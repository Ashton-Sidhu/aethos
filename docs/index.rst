
*"A collection of tools for Data Scientists and ML Engineers for them to focus less on how to do the analysis and instead worry about what are the best analytic tools that will help gain the most insights from their data."*

Welcome to PyAutoMl's documentation!
====================================
Py-automl is a library/platform that automates your data science and analytical tasks at any stage in the pipeline. Py-automl is, at its core, a wrapper that helps automate analytical techniques from various libaries such as pandas, sci-kit learn, gensim, etc. and tries to the bridge the gap 

Py-automl makes it easy to PoC, experiment and compare different techniques and models from various libraries. From cleaning your data, visualizing it and even applying feature engineering techniques from your favourite libraries - all done with a single, human readable, line of code!

For more info such as features, development plan, status and vision checkout the `Py-automl github page <https://github.com/Ashton-Sidhu/py-automl/>`_.

.. toctree::
   :maxdepth: 2
   :caption: Contents:

Usage
=====

To start, we need to import the data science workflow stages as well as pandas.

Before that, we can create a full data science folder structure by running `pyautoml create` from the command line and follow the command prompts.

General Usage
=============

.. code:: python

    from pyautoml import Clean, Preprocess, Feature
    import pandas as pd

    x_train = pd.read_csv('data/train.csv') # load data into pandas

    # Initialize cleaning object with training data
    # By default, if no test data (x_test) is provided, then the data is split with 20% going to the test set
    # Specify predictor field as 'Survived'
    # Specify report name
    clean = Clean(x_train=x_train, target_field='Survived', report_name='Titanic')

    clean.x_train # View your training data
    clean.x_test # View your testing data

    clean # Glance at your training data

    clean[clean.Age > 25] # Filter the data

    clean.new_col = [1, 2] # Add a new column to the data, based off the length of the data provided, it will add it to the train or test set.
    clean['new_col'] = [1, 2] # Another way

    clean.x_train['new_col'] = [1,2] # This is the exact same as the either of code above
    clean.x_test['new_col'] = [1,2]

    clean.data_report(title='Titanic Summary', output_file='titanic_summary.html') # Automate EDA with pandas profiling with an autogenerated report

    clean.describe() # Display a high level view of your data using an extended version of pandas describe

    clean.describe_column('Fare') # Get indepth statistics about the 'Fare' column

    clean.missing_data # View your missing data at anytime

**NOTE:** One of the benefits of using ``pyautoml`` is that any method you apply on your train set, gets applied to your test dataset. For any method that requires fitting (replacing missing data with mean), the method is fit on the training data and then applied to the testing data to avoid data leakage.

**NOTE:** If you are providing a list or a Series and your data is split into train and test, the new column is created in the dataset that matches the length of the data provided. If the length of the data provided matches both train and test data it is added to both. To individually add new columns you can do the following:

Cleaning
========

.. code:: python

    clean.checklist() # Will provide an iteractive checklist to keep track of your cleaning tasks

    clean.replace_missing_mostcommon('Fare', 'Embarked') # Replace missing values in the 'Fare' and 'Embarked' column with the most common values in each of the respective columns.

    rep_mcommon = clean.replace_missing_mostcommon('Fare', 'Embarked') # To create a "checkpoint" of your data (i.e. if you just want to test this analytical method), assign it to a variable

    # Now I can keep going with my analysis using the clean object and if something goes wrong when exploring this analysis path, I can pick right up from this point by using the `rep_mcommon` variable, without having to restart any kernels or reload any data.

    clean.replace_missing_random_discrete('Age') # Replace missing values in the 'Age' column with a random value that follows the probability distribution of the 'Age' column in the training set. 

    clean.drop('Cabin') # Drop the cabin column

    # Columns can also be dropped by defining the columns you want to keep (drop all columns except the ones you want to keep) or by passing in a regex expressions and all columns that match the regex expression will be dropped.

    # As you've started to notice, alot of tasks to clean the data and to explore the data have been reduced down to one command, and are also customizable by providing the respective keyword arguments (see documentation).

    clean.visualize_barplot('Age', 'Survived', groupby='Age', method='mean', xlabel='Age') # Create a barblot of the mean surivial rate grouped by age.

Preprocessing and Feature Engineering
====================================

.. code:: python

    prep = Preprocess(clean) # To move onto preprocessing

    feature = Feature(clean) # to move onto feature engineering

    feature.onehot_encode('Person', 'Embarked', drop_col=True) # One hot encode these columns and then drop the original columns

**NOTE:** In pandas you'll often see ``df = df.method(...)`` or ``df.method(..., inplace=True)`` when transforming your data. Then depending on how you developed your analysis, when a mistake is made you either have to restart the kernel or reload your data entirely. In ``pyautoml`` most methods will change the data inplace (methods that have the keyword argument ``new_col_name`` will create a new column) without having to go ``df = df.method(...)``. To create a "checkpoint" that creates a copy of your current state just assign the method to a variable.

Modelling
=========

.. code:: python

    model = Model(feature) # To move onto modelling

    # Models can be run in various ways

    model.logistic_regression(random_state=42, run=True) # Train a logistic regression model
    model.logistic_regression(gridsearch={'penalty': ['l1', 'l2']}, random_state=42, run=True) # Running gridsearch with the best params

    model.logistic_regression(cv=5, learning_curve=True) # Crossvalidates a logistic regression model and displays the scores and the learning curve

    model.logistic_regression(random_state=42) # Adds a logistic regression model to the queue
    model.random_forest() # Adds a random forest model to the queue
    model.xgboost_classification() # Adds an xgboost classification model to the queue

    model.run_models() # This will run all queued models in parallel
    model.run_models(method='series') # Run each model one after the other

    model.compare_models() # This will display each model evaluated against every metric on the test set

    # Every model is accessed by a unique name that is assiged when you run the model. Default model names can be seen in the function header of each model. 

    model.log_reg.confusion_matrix() # Displays a confusion matrix for the logistic regression model

    model.rf_cls.confusion_matrix() # Displays a confusion matrix for the random forest model

In terms of speed, on the backend I am doing everything I can do to use vectorization to reduce processing and computation time (even when using .apply) and I am constantly trying to make speed improvements where possible.

Installation
============

For package use:

``pip install py-automl``

Currently working on condas implementation.

General Methods and Visualizations
==================================

For general methods and visualization docmentation click `here <https://py-automl.readthedocs.io/en/latest/source/pyautoml.html#module-pyautoml.base/>`_.

Cleaning Methods
================

For cleaning docmentation click `here <https://py-automl.readthedocs.io/en/latest/source/pyautoml.cleaning.html#module-contents/>`_.

Preprocessing Methods
=====================

For preprocessing docmentation click `here <https://py-automl.readthedocs.io/en/latest/source/pyautoml.preprocessing.html#module-pyautoml.preprocessing/>`_.

Feature Engineering Methods
===========================

For feature engineering docmentation click `here <https://py-automl.readthedocs.io/en/latest/source/pyautoml.feature_engineering.html#module-pyautoml.feature_engineering/>`_.

Modelling Methods
=================

For modelling docmentation click `here <https://py-automl.readthedocs.io/en/latest/source/pyautoml.modelling.html#module-pyautoml.modelling/>`_.

Model Analysis Methods
======================

For modelling analysis documentation click `here <>

Examples
========

Examples can be viewed `here <https://github.com/Ashton-Sidhu/py-automl/tree/develop/examples/>`_.

Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
